---
title: 2. 特征降维 (Feature Dimensionality Reduction)
date: 2025-12-08 20:00:00 +0800
categories: [学习笔记, 机器学习]
tags: [AI, 特征工程, PCA, 降维, 数学基础]
math: true
mermaid: true
---

在模式识别和机器学习中，机器并不像人类一样看“图”，而是看“数”。一个对象通常由一组特征（Feature）来描述，这组特征组成了**特征向量**。

然而，原始数据（Raw Data）往往维度极高且包含大量噪声。**特征降维**不仅仅是减少计算量，更是设计高效模式识别系统的关键步骤。

## 1. 为什么要降维？(The Curse of Dimensionality)

原始特征通常存在以下三个核心问题，统称为“维度灾难”：

### 1.1 维度灾难 (Data Volume)
特征维度的增加会让计算量呈指数级爆炸。
> **举例**：一张 $160 \times 120$ 的卫星图，如果直接用像素作为特征，维度高达 **19,200 维**。
> 但如果我们提取“面积”、“周长”等几何特征，可能只需要 **<100 维** 就能描述清楚。

### 1.2 冗余与噪声 (Redundancy & Noise)
原始特征中包含很多与分类任务无关的信息（噪声），或者特征之间高度相关（冗余）。这些信息不仅没用，反而会干扰分类器的性能，降低准确率。

### 1.3 计算问题与过拟合 (Overfitting)
* **矩阵病态**：在样本数有限的情况下，高维数据容易导致计算过程中的矩阵病态 (ill-conditioned)，使得数学求解变得困难。
* **泛化能力差**：特征过多而样本过少，极易导致模型**过拟合**（记住了训练集的所有细节和噪声，但在新数据上表现很差）。

---

## 2. 降维的核心逻辑：选择 vs 提取

降维的目标是将 $n$ 维的原始特征，通过某种方式压缩成 $d$ 维的新特征（$d < n$）。这主要有两种截然不同的思路：

| 维度 | **特征选择 (Feature Selection)** | **特征提取 (Feature Extraction)** |
| :--- | :--- | :--- |
| **做法** | 从原始特征中**直接挑选**出一组最有代表性的子集。 | 通过**数学变换**（映射），将高维特征组合成新的低维特征。 |
| **本质** | 是一种**“包含关系”**，不改变原始特征的数值，只是删掉没用的。 | 改变了原来的特征空间，生成了**新的属性**。 |
| **比喻** | **从一本书里挑出几页最重要的读。** | **读完一整本书，写出一篇简短的读后感。** <br>*(读后感是新的概括，不再是书里的原文)* |

---

## 3. 数学本质与权衡

### 3.1 线性变换
特征提取通常通过寻找一个变换矩阵 $W$，做线性变换来实现：

$$Z = W^T X$$

* $X$：$n$ 维原始特征。
* $Z$：$d$ 维新特征。
* $W$：变换矩阵（关键在于找到这个“好”的 $W$）。

### 3.2 难点与权衡 (Trade-off)
降维必然会造成原始信息的损失。算法的难点在于：**如何在降维的同时，尽可能保留那些本质的、有区别性的信息（比如几何结构信息）？**

PPT 中给出了两个等价的推导角度：
1.  **最大可分性**：让投影后的点尽可能散开（方差最大）。
2.  **最近重构性**：让投影后的点尽可能接近原始点（重构误差最小）。

---

## 4. 经典线性降维算法

根据是否利用了样本的标签信息（Label），降维算法主要分为两类：

### 4.1 PCA (主成分分析) - 无监督
PCA 是最经典的无监督降维方法。
* **核心直觉**：数据在哪个方向上**分布得最广（方差最大）**，那个方向包含的信息就越多。
* **优化目标**：最大化投影后数据的方差。
* **数学表达**：
    $$\max_W \text{tr}(W^T X X^T W) \quad \text{s.t.} \quad W^T W = I$$
    即寻找协方差矩阵 $XX^T$ 的特征向量。



### 4.2 LDA (线性判别分析) - 有监督
LDA 利用了类别标签信息。
* **核心直觉**：寻找最能把**不同类别分开**的方向。
* **优化目标**：最大化**类间距离**（不同类别的中心离得越远越好），同时最小化**类内距离**（同类样本聚得越紧越好）。

---

## 5. 总结

特征选择和提取是设计模式识别系统最关键的步骤之一。其最终目的是：**降低维数，减少冗余，从而降低后续分类器设计的难度，并提升模型的推广能力**。
